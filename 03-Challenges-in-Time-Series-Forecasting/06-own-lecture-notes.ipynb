{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split data for pre-processing?\n",
    "\n",
    "Assume that we have a time-series dataset for which we want to perform data pre-processing, which includes the extraction of window & lag features. Later are using previous time-series data points for the computation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Feature Engine imports \n",
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "# Sktime imports\n",
    "from sktime.transformations.series.lag import Lag\n",
    "\n",
    "# Time series data\n",
    "y = ... \n",
    "\n",
    "# Set up the pipeline\n",
    "feature_pipeline = Pipeline([\n",
    "    # 1. Standard scaler learns parameters from the data\n",
    "    ('scaler', StandardScaler()),\n",
    "    # 2. Simpleme imputer (with constant value) does not learn parameters from the data\n",
    "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=666)),\n",
    "    # 3. Lag feature generator looks at historical values to generate features \n",
    "    ('lagger', Lag())\n",
    "    # 4. NaN-remover (could be also handled somewhere else)\n",
    "    (\"nan-remover\", DropMissingData())\n",
    "])\n",
    "\n",
    "# Split the data into training and test\n",
    "train, test = next(TimeSeriesSplit().split(y))\n",
    "\n",
    "# -- Now it comes -- \n",
    "# Train the pipeline using the training data\n",
    "feature_pipeline.fit(y[train])\n",
    "\n",
    "# Apply the transformation to the training data to later train the machine learning model\n",
    "fy_train = feature_pipeline.transform(y[train])\n",
    "\n",
    "# Apply the transformation to generate the test data\n",
    "# - I apply the transformation to all data points (train + test).\n",
    "# - That allows me to generate, e.g., lag-features (using historical values), for ALL test points.\n",
    "# - There are no missing values in the test set features (most likely)\n",
    "fy_test = feature_pipeline.transform(y)[test]  # <-- !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlined above, I eleborated on my idea of how we can use the transformer pipeline to generate the training and test features. \n",
    "\n",
    "#### Performance\n",
    "\n",
    "We might want to somehow restrict the test set pipeline such, that only the needed part of the training set is processed. In this way we can reduce the computation time.\n",
    "\n",
    "#### Data leakage\n",
    "\n",
    "There is a potential data leak between training and test here. That comes in the overlap of between training and test set for the test set lag features. Some of the training data points will be transformed using models which have been used to train them. \n",
    "\n",
    "Whether that is a real problem, is probably depending on the actual feature extraction pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative approach \n",
    "\n",
    "We can do a small modification to the code and make a clear separation between training and test. However, in this case (as we use lag-features) we will \"loose\" some data for training AND testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to generate the test data\n",
    "fy_test = feature_pipeline.transform(y[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feat-eng-ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
